{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2536d-37d1-443c-bdbd-242058e4428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -mm venv growthschool_genai\n",
    "# source growthschool_genai/bin/activate\n",
    "# pip install streamlit transformers sentence-transformers faiss-cpu PyPDF2 ipykernel\n",
    "# python -m ipykernel install --user --name=growthschool_genai\n",
    "# jupyter lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec7c0f3-d4ba-4b52-a225-a28a3e2090c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raamraam/codes/GenAIWorkshop/genai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# https://ollama.com/download\n",
    "# https://github.com/ollama/ollama-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e56a6f9-6cf2-4f8d-a532-c1d9f478d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_object(obj):\n",
    "    \"\"\"\n",
    "    Helper function to serialize custom objects like EvalResult.\n",
    "    Converts objects with __dict__ attribute to dictionaries and handles datetime objects.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()  # Convert datetime to ISO 8601 string\n",
    "    elif hasattr(obj, \"__dict__\"):\n",
    "        return {key: serialize_object(value) for key, value in obj.__dict__.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [serialize_object(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: serialize_object(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        return obj  # Return the value as-is for primitive types\n",
    "\n",
    "def model_info_to_json(model_info):\n",
    "    \"\"\"\n",
    "    Convert a Hugging Face ModelInfo object to a generalized dictionary,\n",
    "    handling non-serializable fields like EvalResult and datetime.\n",
    "    \"\"\"\n",
    "    return serialize_object(model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8381c911-8c49-4906-957a-a1cdba0ca0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def download_required_files(repo_id, local_dir=\"./model_files\"):\n",
    "    \"\"\"\n",
    "    Download only the necessary files for quickly loading a Hugging Face model.\n",
    "    \n",
    "    Args:\n",
    "        repo_id (str): The Hugging Face model repository ID (e.g., \"bert-base-uncased\").\n",
    "        local_dir (str): The directory where the files will be saved.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # List of required files\n",
    "    required_files = [\n",
    "        \"vocab.txt\",          # Vocabulary file (if applicable)\n",
    "        \"vocab.json\",          # Vocabulary file (if applicable)       \n",
    "        \"config.json\",        # Model configuration\n",
    "        \"tokenizer.json\",     # Tokenizer configuration (if applicable)\n",
    "        \"merges.txt\",         # BPE merge rules file (if applicable)\n",
    "        \"pytorch_model.bin\",  # Model weights\n",
    "        \"model.safetensors\",  # Alternative model weights format\n",
    "    ]\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    \n",
    "    # Download only the required files\n",
    "    for file_name in required_files:\n",
    "        try:\n",
    "            print(f\"Attempting to download: {file_name}\")\n",
    "            local_path = hf_hub_download(repo_id=repo_id, filename=file_name, local_dir=local_dir)\n",
    "            print(f\"Saved to: {local_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not download {file_name}: {e}\")\n",
    "    api = HfApi()\n",
    "    with open(f\"{repo_id.split('/')[1]}.json\", \"w\") as json_file:\n",
    "        json_file.write(json.dumps(model_info_to_json(api.model_info(model))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcbf86a-dfe7-4fa1-bb9f-ae3652aa5d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511aabb5-a515-4e5d-b786-3a118c574c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed0c77a-7b2d-4dd7-90c2-38201f5be552",
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[\n",
    "    # \"nlptown/bert-base-multilingual-uncased-sentiment\", # sentiment analysis\n",
    "    # \"facebook/bart-large-cnn\", # summarization\n",
    "    # \"deepset/roberta-base-squad2\", # question-answering\n",
    "    # \"dbmdz/bert-large-cased-finetuned-conll03-english\", # Named Entity Recognition\n",
    "    # \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", # Sentiment Analysis\n",
    "    # \"openai-community/gpt2-large\", #text generation\n",
    "    # \"atharvamundada99/bert-large-question-answering-finetuned-legal\", #question-answering\n",
    "    # \"distilbert/distilbert-base-cased-distilled-squad\", #question-answering\n",
    "    \"facebook/bart-large-mnli\", #classification\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315a317-8919-45de-827e-350558cbc045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download: vocab.txt\n",
      "Could not download vocab.txt: 404 Client Error. (Request ID: Root=1-6778c2d0-6bb5018f019a96be1944e908;e388836d-6ff0-4b31-8dd2-4cba282bccb1)\n",
      "\n",
      "Entry Not Found for url: https://huggingface.co/facebook/bart-large-mnli/resolve/main/vocab.txt.\n",
      "Attempting to download: vocab.json\n",
      "Saved to: models/model.split('/')[1]/vocab.json\n",
      "Attempting to download: config.json\n",
      "Saved to: models/model.split('/')[1]/config.json\n",
      "Attempting to download: tokenizer.json\n",
      "Saved to: models/model.split('/')[1]/tokenizer.json\n",
      "Attempting to download: merges.txt\n",
      "Saved to: models/model.split('/')[1]/merges.txt\n",
      "Attempting to download: pytorch_model.bin\n",
      "Saved to: models/model.split('/')[1]/pytorch_model.bin\n",
      "Attempting to download: model.safetensors\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    download_required_files(model, local_dir=f\"models/model.split('/')[1]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ed06f-87f8-49cf-9224-f9eceea60669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
